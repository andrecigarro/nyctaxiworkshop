# Optimize queries and get data into a canonical form

## Create your AWS Glue Crawler

* Go to AWS Glue on the AWS Console
* On the left pane, go to **Crawlers**
* Select **Add crawler**
 * Enter the name of your Crawler: Ex.: **NYCityTaxiCrawler**
   * **Next...**
   * **Next...**
 * Include path
   * **s3://username-serverless-analytics/demo**
   * **Next...**
 * Add another data store
  * **s3://username-serverless-analytics-output/demo**
  * **Next...**
 * Add another data store
  * **No**
  * **Next...**
 * Create an IAM role
   * AWSGlueServiceRole-**AWSGlueServiceRole-Default**
   * **Next...**
 * Frequency: **Run on demand**
   * **Next...**
 * Add database
   * Database name: **nycitytaxianalysis**
   * Prefix added to tables (optional): **demo_**
 * Finish
 * Now select the created crawler and select **Run crawler**
 * After a minute you should be able to see that 4 new tables were added
 * If you look under **Tables** on the left pane you should see the 4 newly added tables 
 * If you open and look in each of those table definitions, you see the number of rows for each dataset found and that the columns don’t match between tables
 * On the next section we will create an ETL job to move this data into a query-optimized form. 
 
## Create an ETL job

* On the AWS Glue service page, go to Jobs under ETL on the left pane
* You will now create 4 jobs, one for each .csv file from our raw data
## NYCityTaxiFHV
* Click **Add job**
 * Name: **NYCityTaxiFHV**
 * IAM Role: **AWSGlueServiceRole-Default**
 * Type: **Spark**
 * Glue version: **Spark 2.4, Python 3 with improved job startup time (Glue Version 2.0)**
 * This job runs: **A proposed script generated by AWS Glue**
 * S3 path where the script is stored: **s3://username-serverless-analytics-scripts/demo/**
 * Temporary directory: **s3://username-serverless-analytics-tmp/demo/**
  * **Next**
 * Select **demo_fhv**
  * **Next**
 * Select **Change schema**
  * **Next**
 * Select **Create tables in your data target**
  * Data store: **Amazon S3**
  * Format: **Parquet**
  * Target path: **s3://username-serverless-analytics-output/demo**
    * **Next**
 * Change:
  * pickup_datetime to **pickup_date** with format **timestamp**
  * dropoff_datetime to **dropoff_date** with format **timestamp**
  * **Next**
 * On the provided script change:
  * Add the following header:
 
```python
from pyspark.sql.functions import lit
from awsglue.dynamicframe import DynamicFrame
```
  * Find the last call before the the line that start with the datasink. This is the dynamic frame that is being used to write out the data
  * Let’s now convert that to a DataFrame.  Please replace the <DYNAMIC_FRAME_NAME> with the name generated in the script
  
```python
##----------------------------------
#convert to a Spark DataFrame...
customDF = <DYNAMIC_FRAME_NAME>.toDF()
#add a new column for "type"
customDF = customDF.withColumn("type", lit('fhv'))
# Convert back to a DynamicFrame for further processing.
customDynamicFrame = DynamicFrame.fromDF(customDF, glueContext, "customDF_df")
##----------------------------------
```
  * In the last datasink call, change the dynamic frame to point to the new custom dynamic frame created from the Spark DataFrame:

```python
datasink4 = glueContext.write_dynamic_frame.from_options(frame = customDynamicFrame, connection_type = "s3", connection_options = {"path": "s3://<YOURBUCKET/ AND PREFIX/"}, format = "parquet", transformation_ctx = "datasink4")
```
  * Now save and run the ETL.
  
## Optimize queries and get data into a canonical form

 * Go into the Athena Web Console
 * Select the nycitytaxianalysis database
 * Run the following create statement: 
```SQL 
CREATE EXTERNAL TABLE IF NOT EXISTS nycitytaxianalysis.CanonicalData01 (
  type STRING,
  pickup_date TIMESTAMP,
  dropoff_date TIMESTAMP,
  trip_distance DOUBLE,
  total_amount DOUBLE
  
  )
STORED AS PARQUET
LOCATION 's3://username-serverless-analytics-output/demo/'
TBLPROPERTIES ('classification'='parquet')
```
 * Within the Glue Data Catalog, select the new table that you created through Athena under Actions, select “View data”
 * If you do a query on the count per type of taxi, you notice that there is only “fhv” taxi data in the canonical location:
```SQL 
SELECT type, count(*) FROM "nycitytaxianalysis"."canonicaldata01" group by type
```
 * Now lets continue and add the rest of the dataset to this canonical data table
 
## NYCityTaxiGreen
* Click **Add job**
 * Name: **NYCityTaxiGreen**
 * IAM Role: **AWSGlueServiceRole-Default**
 * Type: **Spark**
 * Glue version: **Spark 2.4, Python 3 with improved job startup time (Glue Version 2.0)**
 * This job runs: **A proposed script generated by AWS Glue**
 * S3 path where the script is stored: **s3://username-serverless-analytics-scripts/demo/**
 * Temporary directory: **s3://username-serverless-analytics-tmp/demo/**
  * **Next**
 * Select **demo_green**
  * **Next**
 * Select **Change schema**
  * **Next**
 * Select **Use tables in the data catalog and update your data target**
  * Select: **canonicaldata01**
    * **Next**
 * Check if all mapping makes sense mainly on:
  * **pickup_date** with format **timestamp**
  * **dropoff_date** with format **timestamp**
  * **total_amount** with format **double**
  * **trip_distance** with format **double**
  * **Next**
 * On the provided script change:
  * Add the following header:
 
```python
from pyspark.sql.functions import lit
from awsglue.dynamicframe import DynamicFrame
```
  * Find the last call before the the line that start with the datasink. This is the dynamic frame that is being used to write out the data
  * Let’s now convert that to a DataFrame.  Please replace the <DYNAMIC_FRAME_NAME> with the name generated in the script
   * Usually **dropnullfields3**
  
```python
##----------------------------------
#convert to a Spark DataFrame...
customDF = <DYNAMIC_FRAME_NAME>.toDF()
#add a new column for "type"
customDF = customDF.withColumn("type", lit('green'))
# Convert back to a DynamicFrame for further processing.
customDynamicFrame = DynamicFrame.fromDF(customDF, glueContext, "customDF_df")
##----------------------------------
```
  * In the last datasink call, change the dynamic frame to point to the new custom dynamic frame created from the Spark DataFrame:

```python
datasink4 = glueContext.write_dynamic_frame.from_options(frame = customDynamicFrame, connection_type = "s3", connection_options = {"path": "s3://<YOURBUCKET/ AND PREFIX/"}, format = "parquet", transformation_ctx = "datasink4")
```
  * Now save and run the ETL.
  
## NYCityTaxiHVFHV
* Click **Add job**
 * Name: **NYCityTaxiHVFHV**
 * IAM Role: **AWSGlueServiceRole-Default**
 * Type: **Spark**
 * Glue version: **Spark 2.4, Python 3 with improved job startup time (Glue Version 2.0)**
 * This job runs: **A proposed script generated by AWS Glue**
 * S3 path where the script is stored: **s3://username-serverless-analytics-scripts/demo/**
 * Temporary directory: **s3://username-serverless-analytics-tmp/demo/**
  * **Next**
 * Select **demo_hvfhv**
  * **Next**
 * Select **Change schema**
  * **Next**
 * Select **Use tables in the data catalog and update your data target**
  * Select: **canonicaldata01**
    * **Next**
 * Check if all mapping makes sense mainly on:
  * **pickup_date** with format **timestamp**
  * **dropoff_date** with format **timestamp**
  * **total_amount** with format **double**
  * **trip_distance** with format **double**
  * **Next**
 * On the provided script change:
  * Add the following header:
 
```python
from pyspark.sql.functions import lit
from awsglue.dynamicframe import DynamicFrame
```
  * Find the last call before the the line that start with the datasink. This is the dynamic frame that is being used to write out the data
  * Let’s now convert that to a DataFrame.  Please replace the <DYNAMIC_FRAME_NAME> with the name generated in the script
   * Usually **dropnullfields3**
  
```python
##----------------------------------
#convert to a Spark DataFrame...
customDF = <DYNAMIC_FRAME_NAME>.toDF()
#add a new column for "type"
customDF = customDF.withColumn("type", lit('hvfhv'))
# Convert back to a DynamicFrame for further processing.
customDynamicFrame = DynamicFrame.fromDF(customDF, glueContext, "customDF_df")
##----------------------------------
```
  * In the last datasink call, change the dynamic frame to point to the new custom dynamic frame created from the Spark DataFrame:

```python
datasink4 = glueContext.write_dynamic_frame.from_options(frame = customDynamicFrame, connection_type = "s3", connection_options = {"path": "s3://<YOURBUCKET/ AND PREFIX/"}, format = "parquet", transformation_ctx = "datasink4")
```
  * Now save and run the ETL.
  
## NYCityTaxiYellow
* Click **Add job**
 * Name: **NYCityTaxiYellow**
 * IAM Role: **AWSGlueServiceRole-Default**
 * Type: **Spark**
 * Glue version: **Spark 2.4, Python 3 with improved job startup time (Glue Version 2.0)**
 * This job runs: **A proposed script generated by AWS Glue**
 * S3 path where the script is stored: **s3://username-serverless-analytics-scripts/demo/**
 * Temporary directory: **s3://username-serverless-analytics-tmp/demo/**
  * **Next**
 * Select **demo_yellow**
  * **Next**
 * Select **Change schema**
  * **Next**
 * Select **Use tables in the data catalog and update your data target**
  * Select: **canonicaldata01**
    * **Next**
 * Check if all mapping makes sense mainly on:
  * **pickup_date** with format **timestamp**
  * **dropoff_date** with format **timestamp**
  * **total_amount** with format **double**
  * **trip_distance** with format **double**
  * **Next**
 * On the provided script change:
  * Add the following header:
 
```python
from pyspark.sql.functions import lit
from awsglue.dynamicframe import DynamicFrame
```
  * Find the last call before the the line that start with the datasink. This is the dynamic frame that is being used to write out the data
  * Let’s now convert that to a DataFrame.  Please replace the <DYNAMIC_FRAME_NAME> with the name generated in the script
   * Usually **dropnullfields3**
  
```python
##----------------------------------
#convert to a Spark DataFrame...
customDF = <DYNAMIC_FRAME_NAME>.toDF()
#add a new column for "type"
customDF = customDF.withColumn("type", lit('yellow'))
# Convert back to a DynamicFrame for further processing.
customDynamicFrame = DynamicFrame.fromDF(customDF, glueContext, "customDF_df")
##----------------------------------
```
  * In the last datasink call, change the dynamic frame to point to the new custom dynamic frame created from the Spark DataFrame:

```python
datasink4 = glueContext.write_dynamic_frame.from_options(frame = customDynamicFrame, connection_type = "s3", connection_options = {"path": "s3://<YOURBUCKET/ AND PREFIX/"}, format = "parquet", transformation_ctx = "datasink4")
```
  * Now save and run the ETL.
  * After the third ETL job completes, you can query the single dataset, in a query-optimized form on S3:
  
```SQL 
SELECT type, count(*) FROM "nycitytaxianalysis"."canonicaldata01" group by type
```
